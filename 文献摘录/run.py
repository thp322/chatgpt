'''
2025.7.23
Harper
cd "D:/pythonProject/æ¥å•/2025.7.23 æ–‡çŒ®æ‘˜å½•"
streamlit run run.py
'''

import openai
import streamlit as st
import pandas as pd
import os
import PyPDF2
import io
from openai import OpenAI

def extract_text_from_pdf(pdf_file):
    try:
        pdf_reader = PyPDF2.PdfReader(pdf_file)
        text = ""
        for page in pdf_reader.pages:
            text += page.extract_text() + "\n"
        return text
    except Exception as e:
        st.error(f"PDFè§£æé”™è¯¯: {str(e)}")
        return None

def truncate_text_for_analysis(text, max_chars=3000):
    if len(text) <= max_chars:
        return text
    
    # å°è¯•ä»æ–‡æ¡£å¼€å¤´æˆªå–
    # æŸ¥æ‰¾ç¬¬ä¸€ä¸ªæ®µè½ç»“æŸæˆ–å…³é”®è¯å‡ºç°çš„ä½ç½®
    first_paragraph_end = text.find('\n\n')
    if first_paragraph_end != -1 and first_paragraph_end < max_chars:
        # å¦‚æœç¬¬ä¸€æ®µå¾ˆçŸ­ï¼Œç»§ç»­åŒ…å«ç¬¬äºŒæ®µ
        second_paragraph_end = text.find('\n\n', first_paragraph_end + 2)
        if second_paragraph_end != -1 and second_paragraph_end < max_chars:
            truncated_text = text[:second_paragraph_end]
        else:
            truncated_text = text[:first_paragraph_end]
    else:
        # å¦‚æœç¬¬ä¸€æ®µå¾ˆé•¿ï¼Œç›´æ¥æˆªå–å‰max_charsä¸ªå­—ç¬¦
        truncated_text = text[:max_chars]
    
    # å¦‚æœæˆªå–åæ–‡æœ¬ä¸å®Œæ•´ï¼Œå°è¯•æ‰¾åˆ°å¥å­ç»“æŸä½ç½®
    if len(truncated_text) == max_chars:
        # æŸ¥æ‰¾æœ€åä¸€ä¸ªå¥å·æˆ–æ¢è¡Œç¬¦
        last_period = truncated_text.rfind('ã€‚')
        last_newline = truncated_text.rfind('\n')
        last_dot = truncated_text.rfind('.')
        
        end_pos = max(last_period, last_newline, last_dot)
        if end_pos > max_chars * 0.8:  # å¦‚æœæ‰¾åˆ°çš„ç»“æŸä½ç½®åœ¨80%ä¹‹å
            truncated_text = truncated_text[:end_pos + 1]
    
    return truncated_text

def extract_fields(text):
    os.environ["http_proxy"] = "http://"
    os.environ["https_proxy"] = "http://"
    client = OpenAI(api_key="api_key", base_url="https://api.poixe.com/v1")
    
    # æ™ºèƒ½æˆªå–æ–‡æœ¬ï¼Œé¿å…è¶…å‡ºtokené™åˆ¶
    truncated_text = truncate_text_for_analysis(text)
    
    prompt = f"""
è¯·ä»”ç»†åˆ†æä¸‹é¢è¿™æ®µæ–‡çŒ®å†…å®¹ï¼Œæå–ä»¥ä¸‹å­—æ®µä¿¡æ¯ã€‚

ç‰¹åˆ«æ³¨æ„ï¼š
1. æ ‡é¢˜é€šå¸¸åœ¨æ–‡ç« æœ€å¼€å¤´ï¼Œå¯èƒ½æ˜¯ä¸­æ–‡æˆ–è‹±æ–‡ï¼Œé€šå¸¸æ¯”è¾ƒé•¿ä¸”æè¿°æ€§å¼º
2. ä½œè€…ä¿¡æ¯é€šå¸¸åœ¨æ ‡é¢˜ä¸‹æ–¹æˆ–æ–‡ç« å¼€å¤´ï¼Œå¯èƒ½æ˜¯ä¸­æ–‡åæˆ–è‹±æ–‡åï¼Œå¤šä¸ªä½œè€…ç”¨åˆ†å·åˆ†éš”
3. å¦‚æœæŸä¸ªå­—æ®µåœ¨æ–‡æœ¬ä¸­æ‰¾ä¸åˆ°ï¼Œè¯·å¡«å†™"æœªæ‰¾åˆ°"

è¯·æå–ä»¥ä¸‹å­—æ®µï¼š
1. ä½œè€… - æå–æ‰€æœ‰ä½œè€…å§“åï¼Œå¤šä¸ªä½œè€…ç”¨åˆ†å·åˆ†éš”ï¼Œå¦‚"å¼ ä¸‰;æå››;ç‹äº”"
2. å›½å®¶ - ä½œè€…æ‰€å±å›½å®¶æˆ–æœºæ„æ‰€åœ¨å›½å®¶
3. æ ‡é¢˜ - æ–‡ç« çš„å®Œæ•´æ ‡é¢˜ï¼Œé€šå¸¸æ¯”è¾ƒé•¿
4. æ–‡çŒ®ç±»å‹ - å¦‚ï¼šç ”ç©¶è®ºæ–‡ã€ç»¼è¿°ã€æ¡ˆä¾‹æŠ¥å‘Šç­‰
5. ç ”ç©¶ç›®æ ‡ - ç ”ç©¶çš„ä¸»è¦ç›®çš„
6. ç ”ç©¶æ–¹æ³• - é‡‡ç”¨çš„ç ”ç©¶æ–¹æ³•
7. ç ”ç©¶å¯¹è±¡ - ç ”ç©¶çš„äººç¾¤æˆ–æ ·æœ¬
8. æ ·æœ¬é‡ - å‚ä¸ç ”ç©¶çš„æ ·æœ¬æ•°é‡
9. æ˜¯å¦å¯¹ç…§ - æ˜¯å¦æœ‰å¯¹ç…§ç»„
10. å¹²é¢„æªæ–½ - é‡‡ç”¨çš„å¹²é¢„æ–¹æ³•
11. è¯„ä»·æŒ‡æ ‡ - è¯„ä»·çš„æŒ‡æ ‡
12. è¯„ä»·å·¥å…· - ä½¿ç”¨çš„è¯„ä»·å·¥å…·
13. è¯„ä»·æ—¶é—´ - è¯„ä»·çš„æ—¶é—´ç‚¹
14. ç»Ÿè®¡å­¦æ–¹æ³• - ä½¿ç”¨çš„ç»Ÿè®¡æ–¹æ³•
15. ç»“æœ - ä¸»è¦ç ”ç©¶ç»“æœ
16. ä¸»è¦å‘ç° - é‡è¦å‘ç°
17. ç ”ç©¶å±€é™æ€§ - ç ”ç©¶çš„å±€é™æ€§

è¯·ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¾“å‡ºï¼Œç¡®ä¿æ‰€æœ‰å­—æ®µéƒ½æœ‰å€¼ï¼ˆæ‰¾ä¸åˆ°æ—¶å¡«å†™"æœªæ‰¾åˆ°"ï¼‰ã€‚

åŸæ–‡å¦‚ä¸‹ï¼š
{truncated_text}
"""
    response = client.chat.completions.create(
        # model="gpt-4",
        # model="claude-3-5-haiku-20241022:free",
        model="gpt-4.1-nano:free",
        messages=[
            {"role": "user", "content": prompt}
        ],
        temperature=0
    )

    import json
    try:
        output = response.choices[0].message.content
        result = json.loads(output)
    except:
        result = {key: "" for key in [
            "ä½œè€…", "å›½å®¶", "æ ‡é¢˜", "æ–‡çŒ®ç±»å‹", "ç ”ç©¶ç›®æ ‡", "ç ”ç©¶æ–¹æ³•",
            "ç ”ç©¶å¯¹è±¡", "æ ·æœ¬é‡", "æ˜¯å¦å¯¹ç…§", "å¹²é¢„æªæ–½", "è¯„ä»·æŒ‡æ ‡",
            "è¯„ä»·å·¥å…·", "è¯„ä»·æ—¶é—´", "ç»Ÿè®¡å­¦æ–¹æ³•", "ç»“æœ", "ä¸»è¦å‘ç°", "ç ”ç©¶å±€é™æ€§"
        ]}
    return result

st.set_page_config(
    page_title="æ–‡çŒ®æ™ºèƒ½æ‘˜å½•åŠ©æ‰‹", 
    layout="wide",
    initial_sidebar_state="collapsed"
)

# éšè—CSS
hide_menu_style = """
        <style>
        #MainMenu {visibility: hidden;}
        footer {visibility: hidden;}
        header {visibility: hidden;}
        </style>
        """
st.markdown(hide_menu_style, unsafe_allow_html=True)
st.title("ğŸ“„ æ–‡çŒ®æ™ºèƒ½æ‘˜å½•åŠ©æ‰‹")

# åˆå§‹åŒ–æ•°æ®æ–‡ä»¶
DATA_PATH = "data/extracted_data.csv"
os.makedirs("data", exist_ok=True)

# åŠ è½½æ•°æ®
def load_data():
    try:
        if os.path.exists(DATA_PATH):
            df = pd.read_csv(DATA_PATH, encoding='utf-8')
            return df
        else:
            # åˆ›å»ºæ–°çš„DataFrame
            df_init = pd.DataFrame(columns=[
                "åºå·", "ä½œè€…", "å›½å®¶", "æ ‡é¢˜", "æ–‡çŒ®ç±»å‹", "ç ”ç©¶ç›®æ ‡", "ç ”ç©¶æ–¹æ³•",
                "ç ”ç©¶å¯¹è±¡", "æ ·æœ¬é‡", "æ˜¯å¦å¯¹ç…§", "å¹²é¢„æªæ–½", "è¯„ä»·æŒ‡æ ‡",
                "è¯„ä»·å·¥å…·", "è¯„ä»·æ—¶é—´", "ç»Ÿè®¡å­¦æ–¹æ³•", "ç»“æœ", "ä¸»è¦å‘ç°", "ç ”ç©¶å±€é™æ€§"
            ])
            df_init.to_csv(DATA_PATH, index=False, encoding='utf-8')
            return df_init
    except Exception as e:
        st.error(f"æ•°æ®åŠ è½½å¤±è´¥: {str(e)}")
        # åˆ›å»ºæ–°çš„DataFrameä½œä¸ºå¤‡é€‰
        df_init = pd.DataFrame(columns=[
            "åºå·", "ä½œè€…", "å›½å®¶", "æ ‡é¢˜", "æ–‡çŒ®ç±»å‹", "ç ”ç©¶ç›®æ ‡", "ç ”ç©¶æ–¹æ³•",
            "ç ”ç©¶å¯¹è±¡", "æ ·æœ¬é‡", "æ˜¯å¦å¯¹ç…§", "å¹²é¢„æªæ–½", "è¯„ä»·æŒ‡æ ‡",
            "è¯„ä»·å·¥å…·", "è¯„ä»·æ—¶é—´", "ç»Ÿè®¡å­¦æ–¹æ³•", "ç»“æœ", "ä¸»è¦å‘ç°", "ç ”ç©¶å±€é™æ€§"
        ])
        return df_init

# ä¿å­˜æ•°æ®
def save_data(df):
    try:
        df.to_csv(DATA_PATH, index=False, encoding='utf-8')
        return True
    except Exception as e:
        st.error(f"æ•°æ®ä¿å­˜å¤±è´¥: {str(e)}")
        return False

# åŠ è½½æ•°æ®
df = load_data()

# æ–‡ä»¶ä¸Šä¼ 
st.subheader("ğŸ“ ä¸Šä¼ PDFæ–‡ä»¶")
uploaded_file = st.file_uploader("è¯·é€‰æ‹©PDFæ–‡ä»¶", type=['pdf'], help="æ”¯æŒPDFæ ¼å¼çš„å­¦æœ¯è®ºæ–‡æ–‡ä»¶")

# æ˜¾ç¤ºä¸Šä¼ çš„æ–‡ä»¶ä¿¡æ¯
if uploaded_file is not None:
    st.success(f"âœ… æ–‡ä»¶ä¸Šä¼ æˆåŠŸ: {uploaded_file.name}")
    
    # æå–PDFæ–‡æœ¬
    with st.spinner("æ­£åœ¨è§£æPDFæ–‡ä»¶..."):
        pdf_text = extract_text_from_pdf(uploaded_file)
    
    if pdf_text:
        # æ˜¾ç¤ºæå–çš„æ–‡æœ¬é¢„è§ˆ
        with st.expander("ğŸ“– PDFæ–‡æœ¬é¢„è§ˆï¼ˆå‰500å­—ç¬¦ï¼‰"):
            st.text(pdf_text[:500] + "..." if len(pdf_text) > 500 else pdf_text)
        
        # æ£€æŸ¥æ–‡æœ¬é•¿åº¦å¹¶æ˜¾ç¤ºæˆªå–ä¿¡æ¯
        original_length = len(pdf_text)
        if original_length > 3000:
            st.warning(f"âš ï¸ æ³¨æ„ï¼šPDFå†…å®¹è¾ƒé•¿ï¼ˆ{original_length}å­—ç¬¦ï¼‰ï¼Œç³»ç»Ÿå°†æ™ºèƒ½æˆªå–é‡è¦éƒ¨åˆ†è¿›è¡Œåˆ†æï¼Œä¼˜å…ˆä¿ç•™æ ‡é¢˜ã€ä½œè€…ã€æ‘˜è¦ã€æ–¹æ³•ç­‰å…³é”®ä¿¡æ¯ã€‚")
        
        # æ˜¾ç¤ºå®é™…ç”¨äºåˆ†æçš„æ–‡æœ¬ï¼ˆè°ƒè¯•ç”¨ï¼‰
        with st.expander("ğŸ” å®é™…åˆ†ææ–‡æœ¬ï¼ˆè°ƒè¯•ç”¨ï¼‰"):
            truncated_text = truncate_text_for_analysis(pdf_text)
            st.text(f"åˆ†ææ–‡æœ¬é•¿åº¦: {len(truncated_text)}å­—ç¬¦")
            st.text("å‰10è¡Œå†…å®¹:")
            lines = truncated_text.split('\n')[:10]
            for i, line in enumerate(lines, 1):
                if line.strip():  # åªæ˜¾ç¤ºéç©ºè¡Œ
                    st.text(f"ç¬¬{i}è¡Œ: {line.strip()}")
            st.text("å®Œæ•´æ–‡æœ¬é¢„è§ˆ:")
            st.text(truncated_text[:1000] + "..." if len(truncated_text) > 1000 else truncated_text)
        
        # æ‰‹åŠ¨è¾“å…¥æ ‡é¢˜å’Œä½œè€…
        with st.expander("âœï¸ æ‰‹åŠ¨è¾“å…¥æ ‡é¢˜å’Œä½œè€…ï¼ˆå¯é€‰ï¼‰"):
            st.info("å¦‚æœè‡ªåŠ¨æå–ä¸å‡†ç¡®ï¼Œå¯ä»¥æ‰‹åŠ¨è¾“å…¥æ ‡é¢˜å’Œä½œè€…ä¿¡æ¯")
            manual_title = st.text_input("æ‰‹åŠ¨è¾“å…¥æ ‡é¢˜:", key="manual_title")
            manual_author = st.text_input("æ‰‹åŠ¨è¾“å…¥ä½œè€…:", key="manual_author", help="å¤šä¸ªä½œè€…ç”¨åˆ†å·åˆ†éš”ï¼Œå¦‚ï¼šå¼ ä¸‰;æå››")
        
        # æå–æ–‡çŒ®ä¿¡æ¯æŒ‰é’®
        if st.button("ğŸ” æå–æ–‡çŒ®ä¿¡æ¯"):
            with st.spinner("æ­£åœ¨è°ƒç”¨ ChatGPT è¿›è¡Œæå–æ–‡ç« ä¿¡æ¯..."):
                result = extract_fields(pdf_text)
                
                # å¦‚æœæ‰‹åŠ¨è¾“å…¥äº†æ ‡é¢˜æˆ–ä½œè€…ï¼Œä½¿ç”¨æ‰‹åŠ¨è¾“å…¥çš„å€¼
                if manual_title:
                    result["æ ‡é¢˜"] = manual_title
                if manual_author:
                    result["ä½œè€…"] = manual_author
                
                new_id = len(df) + 1
                result["åºå·"] = new_id
                df = pd.concat([df, pd.DataFrame([result])], ignore_index=True)
                
                # ä¿å­˜æ•°æ®
                if save_data(df):
                    st.success("æå–æˆåŠŸ âœ…")
                    st.info(f"å½“å‰å·²ä¿å­˜ {len(df)} æ¡æ–‡çŒ®è®°å½•")
                else:
                    st.error("æ•°æ®ä¿å­˜å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æƒé™")
    else:
        st.error("PDFæ–‡ä»¶è§£æå¤±è´¥ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼æ˜¯å¦æ­£ç¡®")

# å±•ç¤ºè¡¨æ ¼
st.subheader("ğŸ“Š å·²æå–æ–‡çŒ®æ•°æ®")

# æ•°æ®ç®¡ç†åŠŸèƒ½
col1, col2, col3 = st.columns(3)
with col1:
    st.metric("æ€»æ–‡çŒ®æ•°", len(df))
with col2:
    if st.button("ğŸ”„ é‡æ–°åŠ è½½æ•°æ®"):
        df = load_data()
        st.success("æ•°æ®é‡æ–°åŠ è½½æˆåŠŸ")
with col3:
    if st.button("ğŸ—‘ï¸ æ¸…ç©ºæ‰€æœ‰æ•°æ®"):
        if st.session_state.get('confirm_clear', False):
            df = pd.DataFrame(columns=[
                "åºå·", "ä½œè€…", "å›½å®¶", "æ ‡é¢˜", "æ–‡çŒ®ç±»å‹", "ç ”ç©¶ç›®æ ‡", "ç ”ç©¶æ–¹æ³•",
                "ç ”ç©¶å¯¹è±¡", "æ ·æœ¬é‡", "æ˜¯å¦å¯¹ç…§", "å¹²é¢„æªæ–½", "è¯„ä»·æŒ‡æ ‡",
                "è¯„ä»·å·¥å…·", "è¯„ä»·æ—¶é—´", "ç»Ÿè®¡å­¦æ–¹æ³•", "ç»“æœ", "ä¸»è¦å‘ç°", "ç ”ç©¶å±€é™æ€§"
            ])
            save_data(df)
            st.success("æ•°æ®å·²æ¸…ç©º")
            st.session_state.confirm_clear = False
        else:
            st.session_state.confirm_clear = True
            st.warning("ç‚¹å‡»ç¡®è®¤æ¸…ç©ºæ•°æ®")

if st.session_state.get('confirm_clear', False):
    st.error("âš ï¸ æ­¤æ“ä½œå°†åˆ é™¤æ‰€æœ‰å·²ä¿å­˜çš„æ–‡çŒ®æ•°æ®ï¼Œè¯·å†æ¬¡ç‚¹å‡»æ¸…ç©ºæŒ‰é’®ç¡®è®¤")

st.dataframe(df, use_container_width=True)

# ä¸‹è½½åŠŸèƒ½
st.subheader("ğŸ“¥ ä¸‹è½½æ•°æ®")
col1, col2 = st.columns(2)

with col1:
    # Excel
    try:
        # openpyxl
        excel_buffer = io.BytesIO()
        with pd.ExcelWriter(excel_buffer, engine='openpyxl') as writer:
            df.to_excel(writer, index=False, sheet_name='æ–‡çŒ®æ‘˜å½•ç»“æœ')
        excel_data = excel_buffer.getvalue()
        st.download_button(
            "ğŸ“Š ä¸‹è½½ä¸ºExcel",
            excel_data,
            "æ–‡çŒ®æ‘˜å½•ç»“æœ.xlsx",
            help="Excelæ ¼å¼ï¼Œæ”¯æŒä¸­æ–‡"
        )
    except Exception as e:
        st.error(f"Excelä¸‹è½½å¤±è´¥: {str(e)}")

with col2:
    # JSON
    json_data = df.to_json(orient='records', force_ascii=False, indent=2)
    st.download_button(
        "ğŸ“‹ ä¸‹è½½ä¸ºJSON",
        json_data,
        "æ–‡çŒ®æ‘˜å½•ç»“æœ.json",
        help="JSONæ ¼å¼ï¼Œæ”¯æŒä¸­æ–‡"
    )




