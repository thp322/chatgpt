'''
2025.7.23
Harper

æœ¬åœ°ï¼š
cd "D:/githubæ‰˜ç®¡é¡¹ç›®/chatgpt"
streamlit run run.py

äº‘ç«¯ï¼š
[secrets]
OPENAI_API_KEY = "sk-your-actual-api-key-here"
OPENAI_BASE_URL = "https://api.openai.com/v1"

æ›´æ–°ï¼š
git add .
git commit -m "Fix deployment issues"
git push
'''

import openai
import streamlit as st
import pandas as pd
import os
import PyPDF2
import io
from openai import OpenAI
import json
import socket
import subprocess
import platform
import re

def detect_proxy_ports():
    """è‡ªåŠ¨æ£€æµ‹ä»£ç†ç«¯å£"""
    proxy_ports = []
    
    # å¸¸è§ä»£ç†ç«¯å£åˆ—è¡¨
    common_ports = [10809, 7890, 7891, 1080, 10808, 8080, 8888, 8889, 1087, 1086]
    
    # æ£€æµ‹æœ¬åœ°ä»£ç†ç«¯å£
    for port in common_ports:
        try:
            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            sock.settimeout(1)
            result = sock.connect_ex(('127.0.0.1', port))
            sock.close()
            if result == 0:
                proxy_ports.append(port)
        except:
            continue
    
    return proxy_ports

def get_proxy_config():
    """è·å–ä»£ç†é…ç½®"""
    # é¦–å…ˆæ£€æŸ¥ç¯å¢ƒå˜é‡
    http_proxy = os.environ.get('http_proxy') or os.environ.get('HTTP_PROXY')
    https_proxy = os.environ.get('https_proxy') or os.environ.get('HTTPS_PROXY')
    
    if http_proxy and https_proxy:
        return http_proxy, https_proxy
    
    # è‡ªåŠ¨æ£€æµ‹ä»£ç†ç«¯å£
    proxy_ports = detect_proxy_ports()
    
    if proxy_ports:
        # ä¼˜å…ˆä½¿ç”¨HTTPä»£ç†ç«¯å£
        http_port = None
        for port in proxy_ports:
            if port in [10809, 7890, 7891, 8080, 8888, 8889]:  # HTTPä»£ç†ç«¯å£
                http_port = port
                break
        
        if http_port:
            proxy_url = f"http://127.0.0.1:{http_port}"
            return proxy_url, proxy_url
    
    # é»˜è®¤è¿”å›v2rayNçš„å¸¸ç”¨ç«¯å£
    return "http://127.0.0.1:10809", "http://127.0.0.1:10809"

def set_proxy_environment():
    """è®¾ç½®ä»£ç†ç¯å¢ƒå˜é‡"""
    # åœ¨äº‘éƒ¨ç½²ç¯å¢ƒä¸­ï¼Œé€šå¸¸ä¸éœ€è¦ä»£ç†
    if os.environ.get('STREAMLIT_SERVER_RUN_ON_IP') or os.environ.get('STREAMLIT_SERVER_PORT'):
        return None, None
    
    http_proxy, https_proxy = get_proxy_config()
    os.environ["http_proxy"] = http_proxy
    os.environ["https_proxy"] = https_proxy
    return http_proxy, https_proxy

def extract_text_from_pdf(pdf_file):
    try:
        pdf_reader = PyPDF2.PdfReader(pdf_file)
        text = ""
        for page in pdf_reader.pages:
            text += page.extract_text() + "\n"
        return text
    except Exception as e:
        st.error(f"PDFè§£æé”™è¯¯: {str(e)}")
        return None

def truncate_text_for_analysis(text, max_chars=3000):
    if len(text) <= max_chars:
        return text
    
    # å°è¯•ä»æ–‡æ¡£å¼€å¤´æˆªå–
    # æŸ¥æ‰¾ç¬¬ä¸€ä¸ªæ®µè½ç»“æŸæˆ–å…³é”®è¯å‡ºç°çš„ä½ç½®
    first_paragraph_end = text.find('\n\n')
    if first_paragraph_end != -1 and first_paragraph_end < max_chars:
        # å¦‚æœç¬¬ä¸€æ®µå¾ˆçŸ­ï¼Œç»§ç»­åŒ…å«ç¬¬äºŒæ®µ
        second_paragraph_end = text.find('\n\n', first_paragraph_end + 2)
        if second_paragraph_end != -1 and second_paragraph_end < max_chars:
            truncated_text = text[:second_paragraph_end]
        else:
            truncated_text = text[:first_paragraph_end]
    else:
        # å¦‚æœç¬¬ä¸€æ®µå¾ˆé•¿ï¼Œç›´æ¥æˆªå–å‰max_charsä¸ªå­—ç¬¦
        truncated_text = text[:max_chars]
    
    # å¦‚æœæˆªå–åæ–‡æœ¬ä¸å®Œæ•´ï¼Œå°è¯•æ‰¾åˆ°å¥å­ç»“æŸä½ç½®
    if len(truncated_text) == max_chars:
        # æŸ¥æ‰¾æœ€åä¸€ä¸ªå¥å·æˆ–æ¢è¡Œç¬¦
        last_period = truncated_text.rfind('ã€‚')
        last_newline = truncated_text.rfind('\n')
        last_dot = truncated_text.rfind('.')
        
        end_pos = max(last_period, last_newline, last_dot)
        if end_pos > max_chars * 0.8:  # å¦‚æœæ‰¾åˆ°çš„ç»“æŸä½ç½®åœ¨80%ä¹‹å
            truncated_text = truncated_text[:end_pos + 1]
    
    return truncated_text

def extract_fields(text):
    # è‡ªåŠ¨è®¾ç½®ä»£ç†
    http_proxy, https_proxy = set_proxy_environment()
    
    # æ˜¾ç¤ºä»£ç†ä¿¡æ¯ï¼ˆè°ƒè¯•ç”¨ï¼‰
    if http_proxy:
        st.info(f"ä½¿ç”¨ä»£ç†: {http_proxy}")
    
    # ä»ç¯å¢ƒå˜é‡è·å–APIé…ç½®
    api_key = st.session_state.get('user_api_key') or os.environ.get('OPENAI_API_KEY')
    base_url = os.environ.get('OPENAI_BASE_URL', "https://api.openai.com/v1")
    
    if not api_key:
        st.error("è¯·è®¾ç½®OPENAI_API_KEYç¯å¢ƒå˜é‡")
        return None
    
    client = OpenAI(api_key=api_key, base_url=base_url)
    
    # æ™ºèƒ½æˆªå–æ–‡æœ¬ï¼Œé¿å…è¶…å‡ºtokené™åˆ¶
    truncated_text = truncate_text_for_analysis(text)
    
    # æ£€æµ‹æ–‡æœ¬è¯­è¨€
    language = detect_language(truncated_text)
    
    # æ˜¾ç¤ºè¯­è¨€æ£€æµ‹ç»“æœ
    language_display = "ä¸­æ–‡" if language == "chinese" else "è‹±æ–‡"
    st.info(f"æ£€æµ‹åˆ°æ–‡çŒ®è¯­è¨€: {language_display}")
    
    # æ ¹æ®è¯­è¨€è·å–ç›¸åº”çš„prompt
    prompt = get_prompt_by_language(language, truncated_text)
    
    try:
        response = client.chat.completions.create(
            model="gpt-4",
            messages=[
                {"role": "user", "content": prompt}
            ],
            temperature=0
        )

        output = response.choices[0].message.content
        result = json.loads(output)
    except Exception as e:
        st.error(f"APIè°ƒç”¨å¤±è´¥: {str(e)}")
        result = {key: "" for key in [
            "publication type", "study design", "participants", "sample size", 
            "Evaluation Time Points", "evaluation indicators", "Comparator / Control Group", 
            "intervention", "Intervention Duration", "platform", "Platform Type", 
            "adaptive learning goals", "Underlying Theory", "System Design and Architecture", 
            "adaptive to what or adaptive variables", "Data Acquisition Methods", 
            "Adapted Elements or what is adapted", "adaptive technology/method", 
            "adaptive endpoints", "outcome", "conclusion", "limitation"
        ]}
    return result

def detect_language(text):
    """æ£€æµ‹æ–‡æœ¬è¯­è¨€ï¼ˆä¸­æ–‡æˆ–è‹±æ–‡ï¼‰"""
    # ç»Ÿè®¡ä¸­æ–‡å­—ç¬¦æ•°é‡
    chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
    # ç»Ÿè®¡è‹±æ–‡å­—ç¬¦æ•°é‡
    english_chars = len(re.findall(r'[a-zA-Z]', text))
    
    # å¦‚æœä¸­æ–‡å­—ç¬¦æ•°é‡å¤§äºè‹±æ–‡å­—ç¬¦æ•°é‡ï¼Œè®¤ä¸ºæ˜¯ä¸­æ–‡
    if chinese_chars > english_chars:
        return "chinese"
    else:
        return "english"

def get_prompt_by_language(language, truncated_text):
    """æ ¹æ®è¯­è¨€è·å–ç›¸åº”çš„prompt"""
    if language == "chinese":
        return f"""
è¯·ä»”ç»†åˆ†æä¸‹é¢è¿™æ®µæ–‡çŒ®å†…å®¹ï¼Œæå–ä»¥ä¸‹å­—æ®µä¿¡æ¯ã€‚

è¯·æå–ä»¥ä¸‹å­—æ®µï¼š
1. publication type - æ–‡çŒ®ç±»å‹ï¼ˆä¾‹å¦‚ï¼šä¼šè®®è®ºæ–‡ã€æœŸåˆŠæ–‡ç« /åŸåˆ›æ–‡ç« ã€ç¤¾è®ºã€å­¦ä½è®ºæ–‡ã€ç³»ç»Ÿç»¼è¿°ã€æ–‡çŒ®ç»¼è¿°ã€è®¨è®ºè®ºæ–‡ã€è‡´ç¼–è¾‘çš„ä¿¡ã€ç®€çŸ­é€šè®¯ã€ä¹¦ç±ç« èŠ‚ã€æŠ€æœ¯æŠ¥å‘Šã€ç«‹åœºè®ºæ–‡ã€ç™½çš®ä¹¦ã€æœªæåŠï¼‰
2. study design - ç ”ç©¶è®¾è®¡ï¼ˆä¾‹å¦‚ï¼šå‘å±•æ€§ç ”ç©¶ã€éå®éªŒè®¾è®¡ã€æ¨ªæ–­é¢ç ”ç©¶ã€é˜Ÿåˆ—ç ”ç©¶ã€ç—…ä¾‹å¯¹ç…§ç ”ç©¶ã€å®šæ€§ç ”ç©¶è®¾è®¡ã€çºµå‘è®¾è®¡ã€æ··åˆæ–¹æ³•ç ”ç©¶ã€é˜Ÿåˆ—è®¾è®¡ã€å›é¡¾æ€§ç ”ç©¶ã€å‰ç»æ€§ç ”ç©¶ã€æœªæåŠï¼‰
3. participants - å‚ä¸ç ”ç©¶çš„ä¸ªä½“ç±»å‹æˆ–ç‰¹å¾ï¼ˆä¾‹å¦‚ï¼šæŠ¤ç†å­¦ç”Ÿã€ä½é™¢åŒ»å¸ˆã€æ•™è‚²å·¥ä½œè€…ï¼‰
4. sample size - ç ”ç©¶ä¸­åŒ…å«çš„å‚ä¸è€…æ€»æ•°ï¼ˆä¾‹å¦‚ï¼š42åå­¦ç”Ÿã€120åå‚ä¸è€…ï¼‰
5. Evaluation Time Points - è¯„ä¼°ç ”ç©¶ç»“æœçš„å…·ä½“æ—¶é—´ç‚¹ï¼ˆä¾‹å¦‚ï¼šå‰æµ‹/åæµ‹ã€å¹²é¢„æœŸé—´ã€å¹²é¢„å3ä¸ªæœˆéšè®¿ï¼‰
6. evaluation indicators - ç”¨äºè¯„ä¼°æœ‰æ•ˆæ€§çš„ç»“æœå˜é‡æˆ–æŒ‡æ ‡ï¼ˆä¾‹å¦‚ï¼šæµ‹è¯•åˆ†æ•°ã€èƒ½åŠ›æ°´å¹³ã€è‡ªæˆ‘æ•ˆèƒ½è¯„åˆ†ã€è¡Œä¸ºæ”¹å˜ï¼‰
7. Comparator / Control Group - ä¸å¹²é¢„ç»„è¿›è¡Œæ¯”è¾ƒçš„å‚è€ƒæ¡ä»¶æˆ–ç»„åˆ«
8. intervention - å®æ–½çš„å¹²é¢„æˆ–æ²»ç–—æ–¹æ³•
9. Intervention Duration - å¹²é¢„å®æ–½çš„æ€»æ—¶é•¿
10. platform - ä½¿ç”¨çš„è‡ªé€‚åº”å­¦ä¹ å¹³å°æˆ–ç³»ç»Ÿåç§°ï¼ˆä¾‹å¦‚ï¼šALEKSã€Smart Tutorã€å®šåˆ¶è‡ªé€‚åº”LMSï¼‰
11. Platform Type - è‡ªé€‚åº”å­¦ä¹ ç³»ç»Ÿçš„ç»“æ„æˆ–åŠŸèƒ½ç±»å‹
12. adaptive learning goals - è‡ªé€‚åº”å­¦ä¹ ç³»ç»Ÿæ—¨åœ¨å®ç°çš„æ•™è‚²ç›®æ ‡
13. Underlying Theory - æ„æˆç³»ç»Ÿè®¾è®¡å’Œé€»è¾‘åŸºç¡€çš„æ•™è‚²æˆ–è®¤çŸ¥ç†è®º
14. System Design and Architecture - ä½¿ç”¨çš„è‡ªé€‚åº”å­¦ä¹ ç³»ç»Ÿæˆ–æ™ºèƒ½è¾…å¯¼ç³»ç»Ÿçš„æ•´ä½“ç»“æ„ã€ç»„ä»¶å’ŒæŠ€æœ¯è®¾è®¡
15. adaptive to what or adaptive variables - æä¾›é€‚åº”æ—¶å¯ä»¥è€ƒè™‘ç”¨æˆ·çš„å“ªäº›æ–¹é¢ï¼ˆä¾‹å¦‚ï¼šç”¨æˆ·ç‰¹å¾ã€çŸ¥è¯†æ°´å¹³ã€ç›®æ ‡å’ŒåŠ¨æœºã€åå¥½ã€å­¦ä¹ è¡Œä¸ºã€æƒ…å¢ƒå› ç´ ï¼‰
16. Data Acquisition Methods - ç³»ç»Ÿå¦‚ä½•æ”¶é›†å…³äºå­¦ä¹ è€…çš„æ•°æ®ä»¥æŒ‡å¯¼è‡ªé€‚åº”å†³ç­–
17. Adapted Elements or what is adapted - æ ¹æ®ç”¨æˆ·ç‰¹å¾åŠ¨æ€è°ƒæ•´çš„ç³»ç»Ÿç‰¹å®šç»„ä»¶æˆ–åŠŸèƒ½ï¼ˆä¾‹å¦‚ï¼šå†…å®¹éš¾åº¦çº§åˆ«ã€å‘ˆç°æ ¼å¼ã€å¯¼èˆªç»“æ„ã€åé¦ˆç±»å‹ï¼‰
18. adaptive technology/method - ç”¨äºå®ç°é€‚åº”æ€§çš„è®¡ç®—æ–¹æ³•ã€ç®—æ³•æˆ–æŠ€æœ¯æœºåˆ¶
19. adaptive endpoints - è‡ªé€‚åº”å­¦ä¹ è¿‡ç¨‹ç»“æŸæˆ–ç³»ç»Ÿåœæ­¢è°ƒæ•´å†…å®¹æˆ–åé¦ˆçš„æ¡ä»¶
20. outcome - ç ”ç©¶çš„å…³é”®ç»“æœæˆ–å‘ç°
21. conclusion - ä½œè€…å¯¹ç ”ç©¶æ„ä¹‰çš„æ€»ç»“æˆ–è§£é‡Š
22. limitation - å¯èƒ½å½±å“ç ”ç©¶ç»“æœæœ‰æ•ˆæ€§æˆ–æ™®éæ€§çš„ç ”ç©¶å¼±ç‚¹æˆ–é™åˆ¶

è¯·ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¾“å‡ºï¼Œç¡®ä¿æ‰€æœ‰å­—æ®µéƒ½æœ‰å€¼ï¼ˆæ‰¾ä¸åˆ°æ—¶å¡«å†™"æœªæåŠ"ï¼‰ã€‚

åŸæ–‡å¦‚ä¸‹ï¼š
{truncated_text}
"""
    else:
        return f"""
Please carefully analyze the following literature content and extract the following fields. 

Please extract the following fields:
1. publication type - Type of publication (e.g., conference paper, journal article/Original Article, Editorial, Thesis/Dissertation, Systematic review, literature review, discussion paper, Letter to the Editor, Short Communication, Book Chapter, Technical Report, Position Paper, White Paper, Not mention)
2. study design - Research design used (e.g., developmental research, Non-experimental Design, Cross-sectional Study, Cohort Study, Case-control Study, Qualitative Research Design, Longitudinal Design, mixed method study, Cohort Design, retrospective study, prospective study, Not mention)
3. participants - The type or characteristics of individuals who took part in the study (e.g., nursing students, medical residents, educators)
4. sample size - The total number of participants included in the study (e.g., 42 students, 120 participants)
5. Evaluation Time Points - The specific time points at which the study outcomes were assessed (e.g., pre-test/post-test, during intervention, 3-month post-intervention)
6. evaluation indicators - The outcome variables or indicators used to assess effectiveness (e.g., test scores, competency levels, self-efficacy ratings, behavioral changes)
7. Comparator / Control Group - The reference condition or group against which the intervention group is compared
8. intervention - The intervention or treatment implemented
9. Intervention Duration - The total length of time over which the intervention was implemented
10. platform - The name of the adaptive learning platform or system used (e.g., ALEKS, Smart Tutor, Custom-built Adaptive LMS)
11. Platform Type - The structural or functional type of the adaptive learning system
12. adaptive learning goals - The intended educational outcomes that the adaptive learning system aims to achieve
13. Underlying Theory - The educational or cognitive theories that form the foundation of the system's design
14. System Design and Architecture - The overall structure, components, and technical design of the system
15. adaptive to what or adaptive variables - What aspects of the user can be considered for adaptation (e.g., user characteristics, knowledge level, goals, preferences, learning behavior, contextual factors)
16. Data Acquisition Methods - How the system collects data about the learner to inform adaptive decisions
17. Adapted Elements or what is adapted - The specific components or features that are dynamically adjusted (e.g., content difficulty, presentation format, navigation structure, feedback type)
18. adaptive technology/method - The computational methods, algorithms, or technical mechanisms used for adaptivity
19. adaptive endpoints - The conditions under which the adaptive learning process concludes
20. outcome - The key results or findings of the study
21. conclusion - The authors' summary or interpretation of the study's meaning
22. limitation - The weaknesses or constraints of the study

Please output in JSON format. If a field cannot be found in the text, please write "Not mention".

Original text:
{truncated_text}
"""

st.set_page_config(
    page_title="æ–‡çŒ®æ™ºèƒ½æ‘˜å½•åŠ©æ‰‹", 
    layout="wide",
    initial_sidebar_state="collapsed"
)

# éšè—CSS
hide_menu_style = """
        <style>
        #MainMenu {visibility: hidden;}
        footer {visibility: hidden;}
        header {visibility: hidden;}
        </style>
        """
st.markdown(hide_menu_style, unsafe_allow_html=True)
st.title("ğŸ“„ æ–‡çŒ®æ™ºèƒ½æ‘˜å½•åŠ©æ‰‹")

# ========== æ–°å¢ï¼šAPIå¯†é’¥è¾“å…¥ ========== #
with st.sidebar:
    st.header("ğŸ”‘ OpenAI APIå¯†é’¥è®¾ç½®")
    user_api_key = st.text_input("è¯·è¾“å…¥OpenAI APIå¯†é’¥ï¼š", type="password", value=st.session_state.get('user_api_key', ''))
    if user_api_key:
        st.session_state['user_api_key'] = user_api_key
        st.success("APIå¯†é’¥å·²ä¿å­˜ï¼")
    else:
        st.warning("è¯·åœ¨æ­¤è¾“å…¥æ‚¨çš„OpenAI APIå¯†é’¥ï¼Œå¦åˆ™æ— æ³•ä½¿ç”¨ä¸»è¦åŠŸèƒ½ã€‚")

# ========== ä¸»ä½“åŠŸèƒ½æ˜¾ç¤ºæ§åˆ¶ ========== #
if not st.session_state.get('user_api_key'):
    st.stop()

# ä»£ç†æ£€æµ‹å’Œé…ç½®
with st.expander("ğŸ”§ ä»£ç†è®¾ç½®"):
    # æ£€æµ‹å¯ç”¨ä»£ç†ç«¯å£
    proxy_ports = detect_proxy_ports()
    
    if proxy_ports:
        st.success(f"æ£€æµ‹åˆ°å¯ç”¨ä»£ç†ç«¯å£: {proxy_ports}")
        
        # è‡ªåŠ¨è®¾ç½®ä»£ç†
        http_proxy, https_proxy = set_proxy_environment()
        if http_proxy:
            st.info(f"è‡ªåŠ¨è®¾ç½®ä»£ç†: {http_proxy}")
            
            # æ‰‹åŠ¨é€‰æ‹©ä»£ç†ç«¯å£
            selected_port = st.selectbox(
                "é€‰æ‹©ä»£ç†ç«¯å£:",
                proxy_ports,
                index=proxy_ports.index(int(http_proxy.split(':')[-1])) if http_proxy else 0
            )
            
            if st.button("åº”ç”¨é€‰æ‹©çš„ä»£ç†ç«¯å£"):
                proxy_url = f"http://127.0.0.1:{selected_port}"
                os.environ["http_proxy"] = proxy_url
                os.environ["https_proxy"] = proxy_url
                st.success(f"å·²è®¾ç½®ä»£ç†: {proxy_url}")
    else:
        st.warning("æœªæ£€æµ‹åˆ°ä»£ç†ç«¯å£ï¼Œä½¿ç”¨é»˜è®¤è®¾ç½®")
        # æ‰‹åŠ¨è¾“å…¥ä»£ç†ç«¯å£
        manual_port = st.text_input("æ‰‹åŠ¨è¾“å…¥ä»£ç†ç«¯å£:", value="10809")
        if st.button("è®¾ç½®ä»£ç†ç«¯å£"):
            proxy_url = f"http://127.0.0.1:{manual_port}"
            os.environ["http_proxy"] = proxy_url
            os.environ["https_proxy"] = proxy_url
            st.success(f"å·²è®¾ç½®ä»£ç†: {proxy_url}")

# åˆå§‹åŒ–æ•°æ®æ–‡ä»¶
DATA_PATH = "data/extracted_data.csv"
os.makedirs("data", exist_ok=True)

# åŠ è½½æ•°æ®
def load_data():
    try:
        if os.path.exists(DATA_PATH):
            df = pd.read_csv(DATA_PATH, encoding='utf-8')
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ•°æ®è¿ç§»ï¼ˆä»æ—§çš„ä¸­æ–‡åˆ—åè¿ç§»åˆ°æ–°çš„è‹±æ–‡åˆ—åï¼‰
            old_columns = ["ä½œè€…", "å›½å®¶", "æ ‡é¢˜", "æ–‡çŒ®ç±»å‹", "ç ”ç©¶ç›®æ ‡", "ç ”ç©¶æ–¹æ³•", "ç ”ç©¶å¯¹è±¡", "æ ·æœ¬é‡", "æ˜¯å¦å¯¹ç…§", "å¹²é¢„æªæ–½", "è¯„ä»·æŒ‡æ ‡", "è¯„ä»·å·¥å…·", "è¯„ä»·æ—¶é—´", "ç»Ÿè®¡å­¦æ–¹æ³•", "ç»“æœ", "ä¸»è¦å‘ç°", "ç ”ç©¶å±€é™æ€§"]
            new_columns = ["publication type", "study design", "participants", "sample size", "Evaluation Time Points", "evaluation indicators", "Comparator / Control Group", "intervention", "Intervention Duration", "platform", "Platform Type", "adaptive learning goals", "Underlying Theory", "System Design and Architecture", "adaptive to what or adaptive variables", "Data Acquisition Methods", "Adapted Elements or what is adapted", "adaptive technology/method", "adaptive endpoints", "outcome", "conclusion", "limitation"]
            
            # å¦‚æœæ£€æµ‹åˆ°æ—§åˆ—åï¼Œè¿›è¡Œæ•°æ®è¿ç§»
            if any(col in df.columns for col in old_columns):
                st.warning("æ£€æµ‹åˆ°æ—§ç‰ˆæœ¬æ•°æ®æ ¼å¼ï¼Œæ­£åœ¨è¿›è¡Œæ•°æ®è¿ç§»...")
                
                # åˆ›å»ºæ–°çš„DataFrameï¼Œä¿æŒåºå·åˆ—
                new_df = pd.DataFrame()
                if "åºå·" in df.columns:
                    new_df["åºå·"] = df["åºå·"]
                
                # æ·»åŠ æ–°åˆ—ï¼Œåˆå§‹åŒ–ä¸ºç©ºå€¼
                for col in new_columns:
                    new_df[col] = ""
                
                # å°è¯•æ˜ å°„æ—§æ•°æ®åˆ°æ–°åˆ—ï¼ˆéƒ¨åˆ†æ˜ å°„ï¼‰
                column_mapping = {
                    "æ–‡çŒ®ç±»å‹": "publication type",
                    "ç ”ç©¶æ–¹æ³•": "study design", 
                    "ç ”ç©¶å¯¹è±¡": "participants",
                    "æ ·æœ¬é‡": "sample size",
                    "è¯„ä»·æ—¶é—´": "Evaluation Time Points",
                    "è¯„ä»·æŒ‡æ ‡": "evaluation indicators",
                    "æ˜¯å¦å¯¹ç…§": "Comparator / Control Group",
                    "å¹²é¢„æªæ–½": "intervention",
                    "ç»“æœ": "outcome",
                    "ç ”ç©¶å±€é™æ€§": "limitation"
                }
                
                for old_col, new_col in column_mapping.items():
                    if old_col in df.columns and new_col in new_df.columns:
                        new_df[new_col] = df[old_col]
                
                # ä¿å­˜è¿ç§»åçš„æ•°æ®
                new_df.to_csv(DATA_PATH, index=False, encoding='utf-8')
                st.success("æ•°æ®è¿ç§»å®Œæˆï¼")
                return new_df
            
            return df
        else:
            # åˆ›å»ºæ–°çš„DataFrame
            df_init = pd.DataFrame(columns=[
                "åºå·", "publication type", "study design", "participants", "sample size", 
                "Evaluation Time Points", "evaluation indicators", "Comparator / Control Group", 
                "intervention", "Intervention Duration", "platform", "Platform Type", 
                "adaptive learning goals", "Underlying Theory", "System Design and Architecture", 
                "adaptive to what or adaptive variables", "Data Acquisition Methods", 
                "Adapted Elements or what is adapted", "adaptive technology/method", 
                "adaptive endpoints", "outcome", "conclusion", "limitation"
            ])
            df_init.to_csv(DATA_PATH, index=False, encoding='utf-8')
            return df_init
    except Exception as e:
        st.error(f"æ•°æ®åŠ è½½å¤±è´¥: {str(e)}")
        # åˆ›å»ºæ–°çš„DataFrameä½œä¸ºå¤‡é€‰
        df_init = pd.DataFrame(columns=[
            "åºå·", "publication type", "study design", "participants", "sample size", 
            "Evaluation Time Points", "evaluation indicators", "Comparator / Control Group", 
            "intervention", "Intervention Duration", "platform", "Platform Type", 
            "adaptive learning goals", "Underlying Theory", "System Design and Architecture", 
            "adaptive to what or adaptive variables", "Data Acquisition Methods", 
            "Adapted Elements or what is adapted", "adaptive technology/method", 
            "adaptive endpoints", "outcome", "conclusion", "limitation"
        ])
        return df_init

# ä¿å­˜æ•°æ®
def save_data(df):
    try:
        df.to_csv(DATA_PATH, index=False, encoding='utf-8')
        return True
    except Exception as e:
        st.error(f"æ•°æ®ä¿å­˜å¤±è´¥: {str(e)}")
        return False

# åŠ è½½æ•°æ®
df = load_data()

# æ–‡ä»¶ä¸Šä¼ 
st.subheader("ğŸ“ ä¸Šä¼ PDFæ–‡ä»¶")
uploaded_file = st.file_uploader("è¯·é€‰æ‹©PDFæ–‡ä»¶", type=['pdf'], help="æ”¯æŒPDFæ ¼å¼çš„å­¦æœ¯è®ºæ–‡æ–‡ä»¶")

# æ˜¾ç¤ºä¸Šä¼ çš„æ–‡ä»¶ä¿¡æ¯
if uploaded_file is not None:
    st.success(f"âœ… æ–‡ä»¶ä¸Šä¼ æˆåŠŸ: {uploaded_file.name}")
    
    # æå–PDFæ–‡æœ¬
    with st.spinner("æ­£åœ¨è§£æPDFæ–‡ä»¶..."):
        pdf_text = extract_text_from_pdf(uploaded_file)
    
    if pdf_text:
        # æ˜¾ç¤ºæå–çš„æ–‡æœ¬é¢„è§ˆ
        with st.expander("ğŸ“– PDFæ–‡æœ¬é¢„è§ˆï¼ˆå‰500å­—ç¬¦ï¼‰"):
            st.text(pdf_text[:500] + "..." if len(pdf_text) > 500 else pdf_text)
        
        # æ£€æŸ¥æ–‡æœ¬é•¿åº¦å¹¶æ˜¾ç¤ºæˆªå–ä¿¡æ¯
        original_length = len(pdf_text)
        if original_length > 3000:
            st.warning(f"âš ï¸ æ³¨æ„ï¼šPDFå†…å®¹è¾ƒé•¿ï¼ˆ{original_length}å­—ç¬¦ï¼‰ï¼Œç³»ç»Ÿå°†æ™ºèƒ½æˆªå–é‡è¦éƒ¨åˆ†è¿›è¡Œåˆ†æï¼Œä¼˜å…ˆä¿ç•™æ ‡é¢˜ã€ä½œè€…ã€æ‘˜è¦ã€æ–¹æ³•ç­‰å…³é”®ä¿¡æ¯ã€‚")
        
        # æ˜¾ç¤ºå®é™…ç”¨äºåˆ†æçš„æ–‡æœ¬ï¼ˆè°ƒè¯•ç”¨ï¼‰
        with st.expander("ğŸ” å®é™…åˆ†ææ–‡æœ¬ï¼ˆè°ƒè¯•ç”¨ï¼‰"):
            truncated_text = truncate_text_for_analysis(pdf_text)
            st.text(f"åˆ†ææ–‡æœ¬é•¿åº¦: {len(truncated_text)}å­—ç¬¦")
            
            # æ˜¾ç¤ºè¯­è¨€æ£€æµ‹ç»“æœ
            language = detect_language(truncated_text)
            language_display = "ä¸­æ–‡" if language == "chinese" else "è‹±æ–‡"
            st.info(f"æ£€æµ‹åˆ°æ–‡çŒ®è¯­è¨€: {language_display}")
            
            st.text("å‰10è¡Œå†…å®¹:")
            lines = truncated_text.split('\n')[:10]
            for i, line in enumerate(lines, 1):
                if line.strip():  # åªæ˜¾ç¤ºéç©ºè¡Œ
                    st.text(f"ç¬¬{i}è¡Œ: {line.strip()}")
            st.text("å®Œæ•´æ–‡æœ¬é¢„è§ˆ:")
            st.text(truncated_text[:1000] + "..." if len(truncated_text) > 1000 else truncated_text)
        
        # æå–æ–‡çŒ®ä¿¡æ¯æŒ‰é’®
        if st.button("ğŸ” æå–æ–‡çŒ®ä¿¡æ¯"):
            with st.spinner("æ­£åœ¨è°ƒç”¨ ChatGPT è¿›è¡Œæå–æ–‡ç« ä¿¡æ¯..."):
                result = extract_fields(pdf_text)
                
                new_id = len(df) + 1
                result["åºå·"] = new_id
                df = pd.concat([df, pd.DataFrame([result])], ignore_index=True)
                
                # ä¿å­˜æ•°æ®
                if save_data(df):
                    st.success("æå–æˆåŠŸ âœ…")
                    st.info(f"å½“å‰å·²ä¿å­˜ {len(df)} æ¡æ–‡çŒ®è®°å½•")
                else:
                    st.error("æ•°æ®ä¿å­˜å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æƒé™")
    else:
        st.error("PDFæ–‡ä»¶è§£æå¤±è´¥ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼æ˜¯å¦æ­£ç¡®")

# å±•ç¤ºè¡¨æ ¼
st.subheader("ğŸ“Š å·²æå–æ–‡çŒ®æ•°æ®")

# æ•°æ®ç®¡ç†åŠŸèƒ½
col1, col2, col3 = st.columns(3)
with col1:
    st.metric("æ€»æ–‡çŒ®æ•°", len(df))
with col2:
    if st.button("ğŸ”„ é‡æ–°åŠ è½½æ•°æ®"):
        df = load_data()
        st.success("æ•°æ®é‡æ–°åŠ è½½æˆåŠŸ")
with col3:
    if st.button("ğŸ—‘ï¸ æ¸…ç©ºæ‰€æœ‰æ•°æ®"):
        if st.session_state.get('confirm_clear', False):
            df = pd.DataFrame(columns=[
                "åºå·", "publication type", "study design", "participants", "sample size", 
                "Evaluation Time Points", "evaluation indicators", "Comparator / Control Group", 
                "intervention", "Intervention Duration", "platform", "Platform Type", 
                "adaptive learning goals", "Underlying Theory", "System Design and Architecture", 
                "adaptive to what or adaptive variables", "Data Acquisition Methods", 
                "Adapted Elements or what is adapted", "adaptive technology/method", 
                "adaptive endpoints", "outcome", "conclusion", "limitation"
            ])
            save_data(df)
            st.success("æ•°æ®å·²æ¸…ç©º")
            st.session_state.confirm_clear = False
        else:
            st.session_state.confirm_clear = True
            st.warning("ç‚¹å‡»ç¡®è®¤æ¸…ç©ºæ•°æ®")

if st.session_state.get('confirm_clear', False):
    st.error("âš ï¸ æ­¤æ“ä½œå°†åˆ é™¤æ‰€æœ‰å·²ä¿å­˜çš„æ–‡çŒ®æ•°æ®ï¼Œè¯·å†æ¬¡ç‚¹å‡»æ¸…ç©ºæŒ‰é’®ç¡®è®¤")

st.dataframe(df, use_container_width=True)

# æ•°æ®ç»Ÿè®¡ä¿¡æ¯
if not df.empty:
    st.subheader("ğŸ“ˆ æ•°æ®ç»Ÿè®¡")
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("æ€»æ–‡çŒ®æ•°", len(df))
    with col2:
        # æ£€æŸ¥åˆ—æ˜¯å¦å­˜åœ¨
        if 'study design' in df.columns:
            st.metric("æœ‰ç ”ç©¶è®¾è®¡çš„æ–‡çŒ®", len(df[df['study design'] != '']))
        else:
            st.metric("æœ‰ç ”ç©¶è®¾è®¡çš„æ–‡çŒ®", 0)
    with col3:
        # æ£€æŸ¥åˆ—æ˜¯å¦å­˜åœ¨
        if 'participants' in df.columns:
            st.metric("æœ‰å‚ä¸è€…çš„æ–‡çŒ®", len(df[df['participants'] != '']))
        else:
            st.metric("æœ‰å‚ä¸è€…çš„æ–‡çŒ®", 0)
    with col4:
        # æ£€æŸ¥åˆ—æ˜¯å¦å­˜åœ¨
        if 'outcome' in df.columns:
            st.metric("æœ‰ç»“æœçš„æ–‡çŒ®", len(df[df['outcome'] != '']))
        else:
            st.metric("æœ‰ç»“æœçš„æ–‡çŒ®", 0)

# ä¸‹è½½åŠŸèƒ½
st.subheader("ğŸ“¥ ä¸‹è½½æ•°æ®")
col1, col2 = st.columns(2)

with col1:
    # Excel
    try:
        # openpyxl
        excel_buffer = io.BytesIO()
        with pd.ExcelWriter(excel_buffer, engine='openpyxl') as writer:
            df.to_excel(writer, index=False, sheet_name='æ–‡çŒ®æ‘˜å½•ç»“æœ')
        excel_data = excel_buffer.getvalue()
        st.download_button(
            "ğŸ“Š ä¸‹è½½ä¸ºExcel",
            excel_data,
            "æ–‡çŒ®æ‘˜å½•ç»“æœ.xlsx",
            help="Excelæ ¼å¼ï¼Œæ”¯æŒä¸­æ–‡"
        )
    except Exception as e:
        st.error(f"Excelä¸‹è½½å¤±è´¥: {str(e)}")

with col2:
    # JSON
    json_data = df.to_json(orient='records', force_ascii=False, indent=2)
    st.download_button(
        "ğŸ“‹ ä¸‹è½½ä¸ºJSON",
        json_data,
        "æ–‡çŒ®æ‘˜å½•ç»“æœ.json",
        help="JSONæ ¼å¼ï¼Œæ”¯æŒä¸­æ–‡"
    )
